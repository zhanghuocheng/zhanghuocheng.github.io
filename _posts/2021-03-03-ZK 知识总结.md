---
layout: page
---
## ZK 知识总

![image-20210512154020021](/images/image-20210512154020021.png)

```
zk保证哪cap哪2项 可用，分区容忍行 最终一致性
zk的常用功能：发布订阅/分布式锁

zk的节点类型特点
1 持久节点  除非手动删除，否则一致存在
2 持久顺序节点 在1基础上增加顺序性
3 临时节点 节点的生命周期与客户端回话绑定，和客户端连接失效，同事移除所有临时节点。（断开连接不一定失效）
4 临时顺序节点 在3基础上新增顺序特性

zk角色
leader 事务的调度处理，保证事务顺序
flower 处理非事务请求，转发事务，参与事务的投票，参与选举投票
Observer 3.3.0 引入- 处理客户端的非事务请求，转发事务请求给Leader服务器 不参与任何形式的投票

服务状态是LOOKING、FOLLOWING、LEADING、OBSERVING。
- LOOKING：寻找Leader状态。当服务器处于该状态时，它会认为当前集群中没有Leader，因此需要进入Leader选举状态。
- FOLLOWING：跟随者状态。表明当前服务器角色是Follower。
- LEADING：领导者状态。表明当前服务器角色是Leader。
- OBSERVING：观察者状态。表明当前服务器角色是Observer。

添加机器的方式
- 全部重启：关闭所有Zookeeper服务，修改配置之后启动。不影响之前客户端的会话。
- 逐个重启：在过半存活即可用的原则下，一台机器重启不影响整个集群对外提供服务。这是比较常用的方式。
3.5版本开始支持动态扩容。

## **`ZAB`协议是什么？**

`ZAB 支持崩溃恢复的原子广播协议。
2种模式 崩溃恢复和消息广播。 选举leader过程模式，一个选举之后正常协作模式

选举的过程 leader 得到的票数>之前集群的数量/2

默认选举算法 **`fast paxos`**

如何实现分布式锁 创建临时顺序阶段，没有获取到所得监听前一个节点
成为dubbo的注册中心  zk重启 注册信息会持久化  zkclient  curator 会持久化么
```



CAP是Consistency、Availablity和Partition-tolerance的缩写。分别是指：



1.一致性（Consistency）：每次读操作都能保证返回的是最新数据，在分布式系统中，如果能针对一个数据项的更新执行成功后，所有的用户都可以读到其最新的值，这样的系统就被认为具有严格的一致性。



2.可用性（Availablity）：任何一个没有发生故障的节点，会在合理的时间内返回一个正常的结果，也就是对于用户的每一个请求总是能够在有限的时间内返回结果；



3.分区容忍性（Partition-torlerance）：当节点间出现网络分区（不同节点处于不同的子网络，子网络之间是联通的，但是子网络之间是无法联通的，也就是被切分成了孤立的集群网络），照样可以提供满足一致性和可用性的服务，除非整个网络环境都发生了故障。

## **`ZooKeeper`是什么？**

`ZooKeeper`是一个开放源码的分布式协调服务，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户,因为Google的Chubby的开源实现。

## **`ZooKeeper`能干什么？**

分布式应用程序可以基于`Zookeeper`实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、`Master`选举、分布式锁和分布式队列等功能。

## **`ZooKeeper`保证了如下分布式一致性特性**

- 顺序一致性
- 原子性
- 单一视图
- 可靠性
- 实时性（最终一致性）

## **`ZooKeeper`四种节点**

- PERSISTENT-持久节点除非手动删除，否则节点一直存在于Zookeeper上
- EPHEMERAL-临时节点临时节点的生命周期与客户端会话绑定，一旦客户端会话失效（客户端与zookeeper连接断开不一定会话失效），那么这个客户端创建的所有临时节点都会被移除。
- PERSISTENT_SEQUENTIAL-持久顺序节点基本特性同持久节点，只是增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。
- EPHEMERAL_SEQUENTIAL-临时顺序节点基本特性同临时节点，增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。

## **`ZooKeeper`服务器角色**

## **Leader**

- 事务请求的唯一调度和处理者，保证集群事务处理的顺序性
- 集群内部各服务的调度者

## **Follower**

- 处理客户端的非事务请求，转发事务请求给Leader服务器
- 参与事务请求Proposal的投票
- 参与Leader选举投票

## **Observer**

3.3.0版本以后引入的一个服务器角色，在不影响集群事务处理能力的基础上提升集群的非事务处理能力

- 处理客户端的非事务请求，转发事务请求给Leader服务器
- 不参与任何形式的投票

## **`ZooKeeper`服务工作状态**

服务器具有四种状态，分别是LOOKING、FOLLOWING、LEADING、OBSERVING。

- LOOKING：寻找Leader状态。当服务器处于该状态时，它会认为当前集群中没有Leader，因此需要进入Leader选举状态。
- FOLLOWING：跟随者状态。表明当前服务器角色是Follower。
- LEADING：领导者状态。表明当前服务器角色是Leader。
- OBSERVING：观察者状态。表明当前服务器角色是Observer。

## **集群支持动态添加机器吗？**

其实就是水平扩容了，Zookeeper在这方面不太好。两种方式：

- 全部重启：关闭所有Zookeeper服务，修改配置之后启动。不影响之前客户端的会话。
- 逐个重启：在过半存活即可用的原则下，一台机器重启不影响整个集群对外提供服务。这是比较常用的方式。

3.5版本开始支持动态扩容。

## **`ZAB`和`Paxos`算法的联系与区别？**

- 相同点：

- - 两者都存在一个类似于Leader进程的角色，由其负责协调多个Follower进程的运行
  - Leader进程都会等待**超过半数**的Follower做出正确的反馈后，才会将一个提案进行提交
  - `ZAB`协议中，每个`Proposal中`都包含一个 `epoch`值来代表当前的`Leader`周期，`Paxos`中名字为`Ballot`

- 不同点：`ZAB`用来构建高可用的分布式数据主备系统（`Zookeeper`），`Paxos`是用来构建分布式一致性状态机系统。

## **`ZAB`协议是什么？**

`ZAB`协议是为分布式协调服务`Zookeeper`专门设计的一种支持崩溃恢复的原子广播协议。

`ZAB`协议包括两种基本的模式：崩溃恢复和消息广播。

当整个`zookeeper`集群刚刚启动或者Leader服务器宕机、重启或者网络故障导致不存在过半的服务器与Leader服务器保持正常通信时，所有进程（服务器）进入崩溃恢复模式，首先选举产生新的Leader服务器，然后集群中Follower服务器开始与新的Leader服务器进行数据同步，当集群中超过半数机器与该Leader服务器完成数据同步之后，退出恢复模式进入消息广播模式，Leader服务器开始接收客户端的事务请求生成事物提案来进行事务请求处理。

## **`Zookeeper Watcher` 机制 -- 数据变更通知**

`Zookeeper`允许客户端向服务端的某个`Znode`注册一个`Watcher`监听，当服务端的一些指定事件触发了这个`Watcher`，服务端会向指定客户端发送一个事件通知来实现分布式的通知功能，然后客户端根据`Watcher`通知状态和事件类型做出业务上的改变。

工作机制：

- 客户端注册`watcher`
- 服务端处理`watcher`
- 客户端回调`watcher`

`Watcher`特性总结：

1. 一次性无论是服务端还是客户端，一旦一个`Watcher`被触发，`Zookeeper`都会将其从相应的存储中移除。这样的设计有效的减轻了服务端的压力，不然对于更新非常频繁的节点，服务端会不断的向客户端发送事件通知，无论对于网络还是服务端的压力都非常大。
2. 客户端串行执行客户端`Watcher`回调的过程是一个串行同步的过程。
3. 轻量

- `Watcher`通知非常简单，只会告诉客户端发生了事件，而不会说明事件的具体内容。
- 客户端向服务端注册`Watcher`的时候，并不会把客户端真实的`Watcher`对象实体传递到服务端，仅仅是在客户端请求中使用`boolean`类型属性进行了标记。

![img](https://pic4.zhimg.com/80/v2-83145433989c24200e4f96a094eeedd3_720w.jpg)



1. `watcher event`异步发送`watcher`的通知事件从`server`发送到`client`是异步的，这就存在一个问题，不同的客户端和服务器之间通过`socket`进行通信，由于网络延迟或其他因素导致客户端在不通的时刻监听到事件，由于`Zookeeper`本身提供了`ordering guarantee`，即客户端监听事件后，才会感知它所监视`znode`发生了变化。所以我们使用`Zookeeper`不能期望能够监控到节点每次的变化。`Zookeeper`只能保证最终的一致性，而无法保证强一致性。
2. 注册`watcher getData、exists、getChildren`
3. 触发`watcher create、delete、setData`
4. 当一个客户端连接到一个新的服务器上时，watch将会被以任意会话事件触发。当与一个服务器失去连接的时候，是无法接收到watch的。而当client重新连接时，如果需要的话，所有先前注册过的watch，都会被重新注册。通常这是完全透明的。只有在一个特殊情况下，watch可能会丢失：对于一个未创建的`znode`的exist watch，如果在客户端断开连接期间被创建了，并且随后在客户端连接上之前又删除了，这种情况下，这个watch事件可能会被丢失。

## **`zookeeper`节点宕机如何处理？**

`zookeeper`本身也是集群，推荐配置不少于3个服务器。`zookeeper`自身也要保证当一个节点宕机时，其他节点会继续提供服务。如果是一个Follower宕机，还有2台服务器提供访问，因为`zookeeper`上的数据是有多个副本的，数据并不会丢失；如果是一个Leader宕机，`zookeeper`会选举出新的Leader。`zookeeper`集群的机制是只要超过半数的节点正常，集群就能正常提供服务。只有在`zookeeper`节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。所以3个节点的cluster可以挂掉1个节点(leader可以得到2票>1.5),2个节点的cluster就不能挂掉任何1个节点了(leader可以得到1票<=1);

## **`zookeeper`是如何选举主leader的？**

当leader崩溃或者leader失去大多数的follower，这时zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的Server都恢复到一个正确的状态。Zk的选举算法有两种：一种是基于basic paxos实现的，另外一种是基于fast paxos算法实现的。系统默认的选举算法为**`fast paxos`**。
更多...

![img](https://pic2.zhimg.com/80/v2-20464b82e6d130919b7b1bcd9902629d_720w.jpg)

![img](https://pic4.zhimg.com/80/v2-6ad503c632cf400a9527eda772103903_720w.jpg)

![img](https://pic2.zhimg.com/80/v2-b687be065d4a5c2a90447de490a6b655_720w.jpg)

![img](https://pic1.zhimg.com/80/v2-80dcc84be5852f34a0e84c97930e9ba0_720w.jpg)





# Zookeeper 注册中心

其他可以作为注册中心的方式

```
Nacos
Zookeeper
Multicast
Redis
Simple
```

Zookeeper 注册中心参考手册

[Zookeeper](http://zookeeper.apache.org/) 是 Apache Hadoop 的子项目，是一个树型的目录服务，支持变更推送，适合作为 Dubbo 服务的注册中心，工业强度较高，可用于生产环境，并推荐使用 [1](https://dubbo.apache.org/zh/docs/v2.7/user/references/registry/zookeeper/#fn:1)。

![/user-guide/images/zookeeper.jpg](https://dubbo.apache.org/imgs/user/zookeeper.jpg)

流程说明：

- 服务提供者启动时: 向 `/dubbo/com.foo.BarService/providers` 目录下写入自己的 URL 地址
- 服务消费者启动时: 订阅 `/dubbo/com.foo.BarService/providers` 目录下的提供者 URL 地址。并向 `/dubbo/com.foo.BarService/consumers` 目录下写入自己的 URL 地址
- 监控中心启动时: 订阅 `/dubbo/com.foo.BarService` 目录下的所有提供者和消费者 URL 地址。

支持以下功能：

- 当提供者出现断电等异常停机时，注册中心能自动删除提供者信息
- 当注册中心重启时，能自动恢复注册数据，以及订阅请求
- 当会话过期时，能自动恢复注册数据，以及订阅请求
- 当设置 `<dubbo:registry check="false" />` 时，记录失败注册和订阅请求，后台定时重试
- 可通过 `<dubbo:registry username="admin" password="1234" />` 设置 zookeeper 登录信息
- 可通过 `<dubbo:registry group="dubbo" />` 设置 zookeeper 的根节点，不配置将使用默认的根节点。
- 支持 `*` 号通配符 `<dubbo:reference group="*" version="*" />`，可订阅服务的所有分组和所有版本的提供者

## 使用

在 provider 和 consumer 中增加 zookeeper 客户端 jar 包依赖：

```xml
<dependency>
    <groupId>org.apache.zookeeper</groupId>
    <artifactId>zookeeper</artifactId>
    <version>3.3.3</version>
</dependency>
```

或直接[下载](http://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper)。

Dubbo 支持 zkclient 和 curator 两种 Zookeeper 客户端实现：

**注意:在2.7.x的版本中已经移除了zkclient的实现,如果要使用zkclient客户端,需要自行拓展**

### 使用 zkclient 客户端

从 `2.2.0` 版本开始缺省为 zkclient 实现，以提升 zookeeper 客户端的健壮性。[zkclient](https://github.com/sgroschupf/zkclient) 是 Datameer 开源的一个 Zookeeper 客户端实现。

缺省配置：

```xml
<dubbo:registry ... client="zkclient" />
```

或：

```sh
dubbo.registry.client=zkclient
```

或：

```sh
zookeeper://10.20.153.10:2181?client=zkclient
```

需依赖或直接[下载](http://repo1.maven.org/maven2/com/github/sgroschupf/zkclient)：

```xml
<dependency>
    <groupId>com.github.sgroschupf</groupId>
    <artifactId>zkclient</artifactId>
    <version>0.1</version>
</dependency>
```

### 使用 curator 客户端

从 `2.3.0` 版本开始支持可选 curator 实现。[Curator](https://github.com/apache/curator) 是 Netflix 开源的一个 Zookeeper 客户端实现。

如果需要改为 curator 实现，请配置：

```xml
<dubbo:registry ... client="curator" />
```

或：

```sh
dubbo.registry.client=curator
```

或：

```sh
zookeeper://10.20.153.10:2181?client=curator
```

需依赖或直接下载[curator-framework](https://repo1.maven.org/maven2/org/apache/curator/curator-framework/), [curator-recipes](https://repo1.maven.org/maven2/org/apache/curator/curator-recipes/)：

```xml
<properties>
    <dubbo.version>2.7.8</dubbo.version>
    <zookeeper.version>2.12.0</zookeeper.version>
</properties>

<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-framework</artifactId>
    <version>${zookeeper.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-recipes</artifactId>
    <version>${zookeeper.version}</version>
</dependency>
```

Zookeeper 单机配置:

```xml
<dubbo:registry address="zookeeper://10.20.153.10:2181" />
```

或：

```xml
<dubbo:registry protocol="zookeeper" address="10.20.153.10:2181" />
```

Zookeeper 集群配置：

```xml
<dubbo:registry address="zookeeper://10.20.153.10:2181?backup=10.20.153.11:2181,10.20.153.12:2181" />
```

或：

```xml
<dubbo:registry protocol="zookeeper" address="10.20.153.10:2181,10.20.153.11:2181,10.20.153.12:2181" />
```

同一 Zookeeper，分成多组注册中心:

```xml
<dubbo:registry id="chinaRegistry" protocol="zookeeper" address="10.20.153.10:2181" group="china" />
<dubbo:registry id="intlRegistry" protocol="zookeeper" address="10.20.153.10:2181" group="intl" />
```

## zookeeper 安装

安装方式参见: [Zookeeper安装手册](https://dubbo.apache.org/zh/docs/v2.7/admin/install/zookeeper)，只需搭一个原生的 Zookeeper 服务器，并将 [Quick Start](https://dubbo.apache.org/zh/docs/v2.7/user/quick-start) 中 Provider 和 Consumer 里的 `conf/dubbo.properties` 中的 `dubbo.registry.address` 的值改为 `zookeeper://127.0.0.1:2181` 即可使用。

## 可靠性声明

阿里内部并没有采用 Zookeeper 做为注册中心，而是使用自己实现的基于数据库的注册中心，即：Zookeeper 注册中心并没有在阿里内部长时间运行的可靠性保障，此 Zookeeper 桥接实现只为开源版本提供，其可靠性依赖于 Zookeeper 本身的可靠性。

## 兼容性声明

因 `2.0.8` 最初设计的 zookeeper 存储结构不能扩充不同类型的数据，`2.0.9` 版本做了调整，所以不兼容，需全部改用 `2.0.9` 版本才行，以后的版本会保持兼容 `2.0.9`。`2.2.0` 版本改为基于 zkclient 实现，需增加 zkclient 的依赖包，`2.3.0` 版本增加了基于 curator 的实现，作为可选实现策略。